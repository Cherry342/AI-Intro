{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "loading configuration file generation_config.json from cache at aitextgen/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from aitextgen import aitextgen\n",
    "\n",
    "# Without any parameters, aitextgen() will download, cache, and load the 124M GPT-2 \"small\" model\n",
    "ai = aitextgen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mWhat is sentience?\u001b[0m\n",
      "\n",
      "The basic idea is that you give a person what he or she wants, and they won't want to use something else. Sometimes you create a buffer that allows the person to express what he or she wants, and in this case you can tell the person what to do with it. When a person sees a picture of a tree, he or she can see the image of the tree in front of him or her. This is called the \"cascade effect\".\n",
      "\n",
      "In other words, a person in relationship may see a picture of a tree and think, \"I will take it down and put it on the tree, because I love it so much\".\n",
      "\n",
      "In this case, the person would prefer to use a tree that was a tree, rather than a tree that was a tree. The tree in question would be a tree that was in a stable, and therefore, had the same properties as the one in your relationship.\n",
      "\n",
      "Cascade effect\n",
      "\n",
      "When you create a buffer, you are making a choice that is not in your normal preferences. You can choose to give a person something that is not the same as what he or she wants, or you can choose to give something that is not what he or she\n",
      "==========\n",
      "\u001b[1mWhat is sentience?\u001b[0m\n",
      "\n",
      "When you use a phone, it sends all the signals through your brain. It's a signal that you want to hear. It's a signal that you want to feel. It's a signal that you want to sense.\n",
      "\n",
      "By using headphones, you can make sure you're hearing the same information over and over again.\n",
      "\n",
      "People don't always like things that sound like they're coming from the same place.\n",
      "\n",
      "If you have a car, you may hear it coming from a different place. It may sound like it's coming from a different place, but it's coming from the same place.\n",
      "\n",
      "If you're hearing it from the same place, you're also hearing it from the same place.\n",
      "\n",
      "When you use a phone, you can hear the same information over and over again.\n",
      "\n",
      "You can tell when the sound is coming from the same place.\n",
      "\n",
      "To hear the same information over and over again, you need to be able to hear it from the same place.\n",
      "\n",
      "The different places that you can hear the same information over and over again are the places you want to see it coming from.\n",
      "\n",
      "They're the places where you can hear it coming from.\n",
      "\n",
      "This\n",
      "==========\n",
      "\u001b[1mWhat is sentience?\u001b[0m How is it that a man has the energy to be a person, and the power to be able to act on whatever is threatening him? The answer to this question is: There's no such thing as \"unnecessary pain.\" It's the only thing that's necessary.\n",
      "\n",
      "The question is, how do you handle pain?\n",
      "\n",
      "The answer is that if you have a pain, you're responsible for the pain.\n",
      "\n",
      "The way we handle pain is by being responsible for it. A pain is never something that's going to stop you from doing something else. It's just a part of your being.\n",
      "\n",
      "The pain is not something you're responsible for. It's the pain that's going to happen to you.\n",
      "\n",
      "The pain is something that's going to happen to you.\n",
      "\n",
      "If you don't feel pain, you can't control it. It's not that you're not feeling pain—it's that you're not feeling pain.\n",
      "\n",
      "Passion is the power to have a fearlessness that is permanent. We're supposed to have it, we're supposed to feel it.\n",
      "\n",
      "If you don't feel pain, you can't control it. It's not that you're not feeling pain\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ai.generate(prompt=\"What is sentience?\", n=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ai.generate(promptt=\"Are you AI?\", n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen.utils import GPT2ConfigCPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenizer('input.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 71604.11it/s]\n"
     ]
    }
   ],
   "source": [
    "config= GPT2ConfigCPU()\n",
    "\n",
    "ai= aitextgen(tokenizer_file='aitextgen.tokenizer.json', config=config)\n",
    "data=TokenDataset('input.txt', tokenizer_file= 'aitextgen.tokenizer.json', block_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ECU/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:474: LightningDeprecationWarning: Setting `Trainer(gpus=0)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=0)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ECU/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[1m5,000 steps reached: saving model to /trained_model\u001b[0m                    \n",
      "Loss: 3.510 — Avg: 3.480:  10%|█         | 5000/50000 [03:22<30:20, 24.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in trained_model/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5,000 steps reached: generating sample texts.\u001b[0m                         \n",
      "Loss: 3.510 — Avg: 3.480:  10%|█         | 5000/50000 [03:22<30:20, 24.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========                                                                    \n",
      ",                                                                             \n",
      "From, that name to tench younger,\n",
      "Whose honourable father, or in love,\n",
      "To strge the charge-ning of famp,\n",
      "Have not a fine and gray, and beholded\n",
      "With all\n",
      "==========                                                                    \n",
      "\u001b[1m10,000 steps reached: saving model to /trained_model\u001b[0m                   \n",
      "Loss: 3.200 — Avg: 3.248:  20%|██        | 10000/50000 [06:29<25:59, 25.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in trained_model/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10,000 steps reached: generating sample texts.\u001b[0m                         \n",
      "Loss: 3.200 — Avg: 3.248:  20%|██        | 10000/50000 [06:29<25:59, 25.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========                                                                     \n",
      ":                                                                              \n",
      "Hast thou now.\n",
      "\n",
      "KING RICHARD III:\n",
      "Sirrah, what's your cousin?\n",
      "\n",
      "BUCKINGHAM:\n",
      "Lets!\n",
      "\n",
      "Nurse:\n",
      "Why, what doth you?\n",
      "\n",
      "JULIET:\n",
      "Away,\n",
      "And\n",
      "==========                                                                     \n",
      "Loss: 3.170 — Avg: 3.177:  27%|██▋       | 13500/50000 [08:45<23:40, 25.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ECU/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "Configuration saved in trained_model/generation_config.json\n"
     ]
    }
   ],
   "source": [
    "ai.train(data, batch_size=8,num_steps=50000, generate_every=5000, save_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mROMEO:\u001b[0m\n",
      "I have good to the end of answer\n",
      "At accounty in full of a pupilish.\n",
      "\n",
      "BAPTISTA:\n",
      "See, where you will know your mothers?\n",
      "\n",
      "AUFIDIUS:\n",
      "If\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "I will not take your body.\n",
      "\n",
      "WARWICK:\n",
      "Nay, you,\n",
      "For it not, but so; therefore; and, you'll know,\n",
      "In thing and my garments; and, if you have no\n",
      "the proo\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "See, I have makute to thy husband,\n",
      "O, a trave apught traitorous womb,\n",
      "And thou dreamest thy brother from thy house,\n",
      "And, in the daughter, but by thy fir\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "How now, aside, letters,\n",
      "To chide the state of carms on their fiends.\n",
      "\n",
      "KATHARINA:\n",
      "Not coppy to thy colours:\n",
      "I ambling and ung\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "Thy wife, my lord, I am died to beloces.\n",
      "\n",
      "KING RICHARD III:\n",
      "Why, my lord, will we will speak.\n",
      "\n",
      "KING RICHARD III:\n",
      "Her name, Norfolk, and most fister,\n",
      "Can\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "Good Buckingham!\n",
      "\n",
      "KATHARINA:\n",
      "Amend me as it is.\n",
      "\n",
      "GORGELO:\n",
      "Do you speak, I think you will be cut to be a tide.\n",
      "\n",
      "PAULINA:\n",
      "G\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "Ay, good uncle, my braged love thou slay,\n",
      "But thou art a care to be watch, that I shall brook\n",
      "The sorrow of thy head. Then, as thou didst not, thou hast\n",
      "Be thou ostinted\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "At thou, and as to behold the duke;\n",
      "I will not pear to be unwarthery.\n",
      "\n",
      "GREGORY:\n",
      "Spaline,\n",
      "To tell the fitch of your lands.\n",
      "\n",
      "PROSP\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "I will tell thee well;\n",
      "And, by my wear, to death, and yet wilt thou not stay.\n",
      "\n",
      "KING RICHARD III:\n",
      "I will be smilent, to your soul\n",
      "To sorrowing: and yet to be your faults.\n",
      "P\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "'Twould you are, and yet I washoose; but one that\n",
      "That I was the dire-statteen.\n",
      "\n",
      "PROSPERO:\n",
      "My lord, but any.\n",
      "\n",
      "PROSP:\n",
      "No, my lord,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ai.generate(10, prompt=\"ROMEO:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
